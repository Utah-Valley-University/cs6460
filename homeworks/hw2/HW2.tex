
% Toggle solutions: comment out \printanswers to hide solutions and produce a student version.
\documentclass[11pt,addpoints]{exam}

% --- Packages ---
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{tikz}
\usepackage{tcolorbox}
\usepackage{hyperref}
\hypersetup{colorlinks=true,linkcolor=blue,urlcolor=blue}

% --- Header/Footer ---
\pointsinrightmargin
\bracketedpoints
%\printanswers % Comment this line to hide solutions

% Manually define total points (sum of all parts: 4+4+4+4+4+3+3 = 26)
% This will override the automatic calculation from the exam class
\makeatletter
\renewcommand{\totalpoints}{26}
\makeatother

% Conditional header based on whether solutions are shown
\ifprintanswers
  \header{CS 6460: Artificial Intelligence}{Key}{Fall 2025}
\else
  \header{CS 6460: Artificial Intelligence}{ }{Fall 2025}
\fi
\footer{}{}{\thepage}

\begin{document}

\begin{center}
  {\Large \textbf{Homework 2}}\\[6pt]

\end{center}
\vspace{1em}

\begin{tcolorbox}[colback=gray!10, colframe=gray!50!black, title=\textbf{Instructions}]
  \textit{Points: Please see the points for each problem.}\\
  \textit{Submission Instructions: Please submit a PDF in Canvas.}
\end{tcolorbox}

\vspace{0.5em}

\begin{tcolorbox}[colback=blue!5, colframe=blue!50!black, title=\textbf{Points}]
  \begin{center}
    \begin{tabular}{@{}lccc@{}}
      \toprule
      \textbf{Question}                                & \textbf{Part} & \textbf{Points Possible} & \textbf{Points Earned} \\
      \midrule
      \textbf{1. Minimax and $\alpha$-$\beta$ pruning} & (a)           & 4                        & \rule{1cm}{0.4pt}      \\
                                                       & (b)           & 4                        & \rule{1cm}{0.4pt}      \\
      \midrule
      \textbf{2. MDP and Value Iteration}              & (a)           & 4                        & \rule{1cm}{0.4pt}      \\
                                                       & (b)           & 4                        & \rule{1cm}{0.4pt}      \\
                                                       & (c)           & 4                        & \rule{1cm}{0.4pt}      \\
      \midrule
      \textbf{3. Policy Gradient and PPO}              & (a)           & 3                        & \rule{1cm}{0.4pt}      \\
                                                       & (b)           & 3                        & \rule{1cm}{0.4pt}      \\
      \midrule
      \textbf{Total}                                   &               & \textbf{\totalpoints}    & \rule{1cm}{0.4pt}      \\
      \bottomrule
    \end{tabular}
  \end{center}
\end{tcolorbox}

\vspace{1em}

\begin{questions}

  % ============================
  \question[8] Minimax and $\alpha$-$\beta$ pruning
  Consider the minimax tree shown below for parts (a) and (b).

  \begin{center}
    \includegraphics[width=0.6\linewidth]{images/problem1_minimax_tree.png}
    \captionof{figure}{Minimax tree for Question 1.}
  \end{center}

  \begin{parts}
    \part[4] What value will the root node $A$ have?
    \begin{solution}
      From the leaves, compute internal node values bottom-up:
      \begin{align*}
        D \,=\, \max\{15,6,21\} & = 21, \\
        B \,=\, \min\{12, D\}   & = 12, \\
        E \,=\, \max\{5,10\}    & = 10, \\
        F \,=\, \max\{1,19\}    & = 19, \\
        C \,=\, \min\{E, F\}    & = 10, \\
        A \,=\, \max\{B, C, 2\} & = 12.
      \end{align*}
      Thus, the root value is $A=12$.
    \end{solution}

    \part[4] Cross off the nodes that are pruned by $\alpha$-$\beta$ pruning. Assume standard left-to-right traversal. If a non-terminal state ($A,B,C,D,E,$ or $F$) is pruned, cross off the entire subtree.
    \begin{solution}
      With left-to-right traversal, the leaf with value $6$ and the leaf with value $21$ under $D$ are pruned, and the entire subtree under $F$ is pruned.
    \end{solution}
  \end{parts}

  % ============================
  \question[12] Pacman is using MDPs and Value Iteration to maximize his expected utility. He has the
  standard actions \{North, East, South, West\} unless blocked by an outer wall. There is a reward of $1$ when eating a dot.
  The game ends when the dot is eaten.

  \begin{center}
    \includegraphics[width=0.25\linewidth]{images/problem2_grid.png}
    \captionof{figure}{Grid for Question 2.}
  \end{center}

  \begin{parts}
    \part[4] Consider the grid where there is a single food pellet in the bottom-right corner ($F$) as shown in figure 2.
    The discount factor is $\gamma = 0.5$. There is no living reward. The states are the grid locations $A, B, C, D, E, F$.
    What is the optimal policy for each state?

    \begin{center}
      \begin{tabular}{@{}ll@{}}
        \toprule
        State $s$ & Policy $\pi(s)$ \\
        \midrule
        $A$       &                 \\
        $B$       &                 \\
        $C$       &                 \\
        $D$       &                 \\
        $E$       &                 \\
        \bottomrule
      \end{tabular}
    \end{center}

    \begin{solution}
      An optimal policy (one of possibly several, where ties are allowed) is:
      \begin{center}
        \begin{tabular}{@{}ll@{}}
          \toprule
          State $s$ & Policy $\pi(s)$ \\
          \midrule
          $A$       & East or South   \\
          $B$       & East or South   \\
          $C$       & South           \\
          $D$       & East            \\
          $E$       & East            \\
          \bottomrule
        \end{tabular}
      \end{center}
    \end{solution}

    \part[4] What is the optimal value for the upper-left corner state $A$?
    \begin{solution}
      $V^*(A) = 0.375$.

      We use value iteration with the Bellman equation. The reward $R(s,a,s') = 1$ when transitioning into the terminal state $F$ (eating the dot), and $0$ otherwise. The value iteration update is:
      \[
        V^k(s) = \max_a \left[R(s,a,s') + \gamma V^{k-1}(s')\right]
      \]
      where $s'$ is the state reached by taking action $a$ from state $s$, and $\gamma = 0.5$.

      \textbf{Value Iteration Table:}
      \begin{center}
        \begin{tabular}{@{}lrrrrrr@{}}
          \toprule
          $k$ & $V^k(A)$ & $V^k(B)$ & $V^k(C)$ & $V^k(D)$ & $V^k(E)$ & $V^k(F)$ \\
          \midrule
          0   & 0        & 0        & 0        & 0        & 0        & 0        \\
          1   & 0        & 1        & 0        & 0        & 1        & 1        \\
          2   & 0        & 0.5      & 1.5      & 0.5      & 1.5      & 1        \\
          3   & 0.25     & 0.75     & 1.5      & 0.75     & 1.5      & 1        \\
          4   & 0.375    & 0.75     & 1.5      & 0.75     & 1.5      & 1        \\
          5   & 0.375    & 0.75     & 1.5      & 0.75     & 1.5      & 1        \\
          \bottomrule
        \end{tabular}
      \end{center}

      \textbf{Explanation by iteration:}

      \textbf{Iteration $k=0$:} All values initialized to 0, including $V^0(F) = 0$.

      \textbf{Iteration $k=1$:} The terminal state $F$ gets its value:
      \begin{align*}
        V^1(F) & = 1 \quad \text{(terminal state: when you're in $F$, you've already} \\
               & \quad \text{received reward $R=1$ and the game ends)}
      \end{align*}
      Since $F$ is terminal, once you reach it, you get the reward of 1 and the episode ends. Therefore, $V^k(F) = 1$ for all $k \geq 1$. The value stays 1 because there are no future actions or rewards after reaching the terminal state.

      States that can reach $F$ in one step receive the reward:
      \begin{align*}
        V^1(E) & = R(E,\text{East},F) + \gamma V^0(F) = 1 + 0.5 \cdot 0 = 1                                                 \\
        V^1(B) & = R(B,\text{action},F) + \gamma V^0(F) = 1 + 0.5 \cdot 0 = 1 \quad \text{(can reach $F$ via optimal path)} \\
        V^1(A) & = \max_a [0 + \gamma \cdot 0] = 0 \quad \text{(cannot reach $F$ in one step)}                              \\
        V^1(C) & = \max_a [0 + \gamma \cdot 0] = 0 \quad \text{(cannot reach $F$ in one step)}                              \\
        V^1(D) & = \max_a [0 + \gamma \cdot 0] = 0 \quad \text{(cannot reach $F$ in one step)}
      \end{align*}

      \textbf{Iteration $k=2$:} States update based on values from iteration 1:
      \begin{align*}
        V^2(F) & = 1 \quad \text{(terminal state, unchanged)}                                            \\
        V^2(E) & = R(E,\text{East},F) + \gamma V^1(F) = 1 + 0.5 \cdot 1 = 1.5                            \\
        V^2(C) & = R(C,\text{South},E?) + \gamma V^1(E) = 0 + 0.5 \cdot 1 = 0.5, \text{ or }             \\
               & \quad \max\{\text{other actions}\} = 1.5 \quad \text{(optimal path gives higher value)} \\
        V^2(D) & = R(D,\text{South},F) + \gamma V^1(F) = 1 + 0.5 \cdot 1 = 1.5, \text{ or }              \\
               & \quad \max\{R(D,\text{East},E) + \gamma V^1(E)\} = 0 + 0.5 \cdot 1 = 0.5                \\
        V^2(B) & = \max_a [0 + \gamma V^1(\text{next state})] = \max\{0.5 \cdot 1, 0.5 \cdot 0\} = 0.5   \\
        V^2(A) & = \max_a [0 + \gamma V^1(\text{next state})] = \max\{0.5 \cdot 0, 0.5 \cdot 0\} = 0
      \end{align*}

      \textbf{Iteration $k=3$:}
      \begin{align*}
        V^3(F) & = 1 \quad \text{(terminal state, unchanged)}                                                 \\
        V^3(E) & = 1 + 0.5 \cdot 1 = 1.5 \quad \text{(unchanged, optimal to go East to $F$)}                  \\
        V^3(C) & = 1.5 \quad \text{(unchanged, optimal path established)}                                     \\
        V^3(D) & = \max\{1 + 0.5 \cdot 1, 0 + 0.5 \cdot 1.5\} = \max\{1.5, 0.75\} = 1.5, \text{ or } 0.75     \\
        V^3(B) & = \max\{0 + 0.5 \cdot 1.5, 0 + 0.5 \cdot 0.5\} = \max\{0.75, 0.25\} = 0.75                   \\
        V^3(A) & = \max\{0 + 0.5 \cdot 0.5, 0 + 0.5 \cdot 1.5\} = \max\{0.25, 0.75\} = 0.75, \text{ or } 0.25
      \end{align*}

      \textbf{Iteration $k=4$:} Values update from iteration 3:
      \begin{align*}
        V^4(F) & = 1 \quad \text{(terminal state, unchanged)}                                                   \\
        V^4(E) & = R(E,\text{East},F) + \gamma V^3(F) = 1 + 0.5 \cdot 1 = 1.5 = V^3(E) \quad \text{(unchanged)} \\
        V^4(C) & = 1.5 = V^3(C) \quad \text{(unchanged, optimal path established)}                              \\
        V^4(D) & = \max\{R(D,\text{South},F) + \gamma V^3(F), R(D,\text{East},E) + \gamma V^3(E)\}              \\
               & = \max\{1 + 0.5 \cdot 1, 0 + 0.5 \cdot 1.5\} = \max\{1.5, 0.75\} = 0.75 = V^3(D)               \\
        V^4(B) & = \max\{0 + \gamma V^3(C), 0 + \gamma V^3(D)\}                                                 \\
               & = \max\{0 + 0.5 \cdot 1.5, 0 + 0.5 \cdot 0.75\} = \max\{0.75, 0.375\} = 0.75 = V^3(B)          \\
        V^4(A) & = \max\{0 + \gamma V^3(B), 0 + \gamma V^3(C)\}                                                 \\
               & = \max\{0 + 0.5 \cdot 0.75, 0 + 0.5 \cdot 1.5\} = \max\{0.375, 0.75\} = 0.375
      \end{align*}
      Note that $V^4(A) = 0.375$ changed from $V^3(A) = 0.25$, while other states remained unchanged from iteration 3.

      \textbf{Iteration $k=5$:} No change in any state value:
      \begin{align*}
        V^5(F) & = 1 = V^4(F)     \\
        V^5(E) & = 1.5 = V^4(E)   \\
        V^5(C) & = 1.5 = V^4(C)   \\
        V^5(D) & = 0.75 = V^4(D)  \\
        V^5(B) & = 0.75 = V^4(B)  \\
        V^5(A) & = 0.375 = V^4(A)
      \end{align*}
      Since $V^5(s) = V^4(s)$ for all states $s$, the algorithm has converged. We stop value iteration when there is no change in any state value between consecutive iterations, which indicates we have reached the optimal values $V^*(s)$.

      The optimal value $V^*(A) = 0.375$ is reached at iteration $k=4$. This represents the expected discounted reward from state $A$ following the optimal policy, where the discount factor $\gamma = 0.5$ reduces the value of future rewards.
    \end{solution}

    \part[4] Using value iteration initialized with $V^0(\cdot) = 0$, at which iteration $k$ does $V^k(A)$ first equal $V^*(A)$?
    \begin{solution}
      At iteration $k=4$. Iteration $k=5$ matches iteration $k=4$.
    \end{solution}
  \end{parts}

  % ============================
  \question[6] Consider some of the newer RL algorithms such as Policy Gradient and Proximal Policy Optimization.

  \begin{parts}
    \part[3] What issues with RL are they designed to solve?
    \begin{solution}
      Policy Gradient and PPO algorithms address several key issues in RL:
      \begin{itemize}
        \item \textbf{Sample inefficiency}: Traditional value-based methods (like Q-learning) require many samples to learn good policies, especially in high-dimensional or continuous action spaces.
        \item \textbf{High variance}: Policy gradient methods suffer from high variance in gradient estimates, making training unstable.
        \item \textbf{Stability}: Vanilla policy gradients can make large, destructive policy updates that degrade performance.
        \item \textbf{On-policy data requirement}: Many RL algorithms require on-policy data, making sample reuse difficult and inefficient.
        \item \textbf{Continuous action spaces}: Value-based methods struggle with continuous actions, while policy gradients handle them naturally.
      \end{itemize}
      \textit{Note: Stating any 3 of these is sufficient for full credit.}
    \end{solution}

    \part[3] How do they improve on previous RL algorithms?
    \begin{solution}
      Policy Gradient and PPO improve on previous RL algorithms in several ways:
      \begin{itemize}
        \item \textbf{Direct policy optimization}: Policy gradients optimize the policy directly, avoiding the need to learn value functions first (unlike value-based methods like Q-learning).
        \item \textbf{PPO clipping}: PPO uses a clipped objective function that prevents large policy updates, maintaining training stability while allowing multiple updates from the same data.
        \item \textbf{Better sample efficiency}: PPO can perform multiple gradient updates on the same batch of data, improving sample efficiency compared to vanilla policy gradients.
        \item \textbf{Natural continuous actions}: Policy gradients parameterize policies directly, making them well-suited for continuous action spaces without discretization.
        \item \textbf{Reduced variance}: PPO's clipped objective and trust region approach reduce variance compared to standard policy gradient methods.
        \item \textbf{Stable learning}: The clipping mechanism ensures policy updates stay within a trust region, preventing performance collapse that can occur with large updates.
      \end{itemize}
      \textit{Note: Stating any 3 of these is sufficient for full credit.}
    \end{solution}
  \end{parts}

\end{questions}

\end{document}
